#!/bin/bash

set -e

echo "ğŸ§ª Testing Kubernetes 1.33 In-Place Pod Resize..."

# Get first pod name
POD_NAME=$(kubectl get pods -l app=resize-demo -o jsonpath='{.items[0].metadata.name}')
echo "ğŸ“Œ Testing with pod: $POD_NAME"

# Function to show current resources
show_resources() {
    echo "ğŸ“Š Current resources for $POD_NAME:"
    kubectl get pod $POD_NAME -o custom-columns=NAME:.metadata.name,CPU-REQ:.spec.containers[0].resources.requests.cpu,MEM-REQ:.spec.containers[0].resources.requests.memory,CPU-LIM:.spec.containers[0].resources.limits.cpu,MEM-LIM:.spec.containers[0].resources.limits.memory
    
    echo "ğŸ” Container status:"
    kubectl get pod $POD_NAME -o jsonpath='{.status.containerStatuses[0].resources}' 2>/dev/null || echo "Resource status not available"
}

# Function to monitor resize status
monitor_resize() {
    echo "ğŸ‘€ Monitoring resize status..."
    for i in {1..30}; do
        STATUS=$(kubectl get pod $POD_NAME -o jsonpath='{.status.conditions[?(@.type=="PodResizeInProgress")].status}' 2>/dev/null || echo "")
        if [ "$STATUS" = "True" ]; then
            echo "ğŸ”„ Resize in progress... ($i/30)"
            sleep 2
        else
            echo "âœ… Resize completed!"
            break
        fi
    done
}

echo "ğŸ¬ Phase 1: Show initial state"
show_resources

echo ""
echo "ğŸ¬ Phase 2: Scale UP CPU and Memory (simulating traffic spike)"
echo "ğŸ“ˆ Increasing CPU: 0.5 -> 1.5, Memory: 512Mi -> 2Gi"
echo "âš ï¸ Note: This respects the LimitRange max values (4 CPU, 8Gi memory)"

kubectl patch pod $POD_NAME --subresource resize -p '{
  "spec": {
    "containers": [{
      "name": "app",
      "resources": {
        "requests": {"cpu": "1.5", "memory": "2Gi"},
        "limits": {"cpu": "2.5", "memory": "3Gi"}
      }
    }]
  }
}' || echo "âŒ Resize blocked by quotas/limits - this is expected behavior!"

monitor_resize
show_resources

echo ""
echo "ğŸ¬ Phase 3: Scale DOWN resources (cost optimization)"
echo "ğŸ“‰ Reducing CPU: 1.5 -> 0.3, Memory: 2Gi -> 256Mi"

kubectl patch pod $POD_NAME --subresource resize -p '{
  "spec": {
    "containers": [{
      "name": "app",
      "resources": {
        "requests": {"cpu": "0.3", "memory": "256Mi"},
        "limits": {"cpu": "0.8", "memory": "512Mi"}
      }
    }]
  }
}'

monitor_resize
show_resources

echo ""
echo "ğŸ¬ Phase 4: Verify pod behavior during resize"
RESTART_COUNT=$(kubectl get pod $POD_NAME -o jsonpath='{.status.containerStatuses[0].restartCount}')
echo "ğŸ”„ Restart count: $RESTART_COUNT"

if [ "$RESTART_COUNT" = "0" ]; then
    echo "ğŸ‰ SUCCESS! Pod resized without restart - Pure in-place resize worked!"
    echo "ğŸ’¡ This means only CPU was scaled (restartPolicy: NotRequired)"
elif [ "$RESTART_COUNT" = "1" ]; then
    echo "âœ… EXPECTED! Pod was restarted once during memory resize"
    echo "ğŸ’¡ Memory scaling used 'RestartContainer' policy for safety"
    echo "ğŸ›¡ï¸ This prevents memory-related crashes (OOM kills)"
    echo "âš¡ CPU scaling was still done in-place without restart"
else
    echo "âš ï¸ Unexpected restart count: $RESTART_COUNT"
    echo "ğŸ’¡ Check your cluster configuration"
fi

echo ""
echo "ğŸ¬ Phase 5: Load test to trigger HPA (optional)"
echo "ğŸ’¡ Run 'kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh' in another terminal"
echo "ğŸ’¡ Then: 'while true; do wget -q -O- http://resize-demo-service; done'"

echo ""
echo "ğŸ¬ Phase 6: Test quota protection (intentional failure)"
echo "ğŸ’€ Attempting to exceed limits (this should FAIL):"

kubectl patch pod $POD_NAME --subresource resize -p '{
  "spec": {
    "containers": [{
      "name": "app",
      "resources": {
        "requests": {"cpu": "5", "memory": "10Gi"},
        "limits": {"cpu": "6", "memory": "12Gi"}
      }
    }]
  }
}' || echo "ğŸ›¡ï¸ GOOD! Resize was blocked by LimitRange protection!"

echo ""
echo "ğŸ¬ Phase 7: Test Kyverno validation (intentional failure)"
echo "ğŸ’€ Attempting resize that violates Kyverno policy:"

kubectl patch pod $POD_NAME --subresource resize -p '{
  "spec": {
    "containers": [{
      "name": "app",
      "resources": {
        "requests": {"cpu": "6", "memory": "12Gi"},
        "limits": {"cpu": "8", "memory": "16Gi"}
      }
    }]
  }
}' || echo "ğŸ›¡ï¸ EXCELLENT! Resize was blocked by Kyverno policy validation!"

echo ""
echo "ğŸ§ª Testing namespace auto-quota generation..."
kubectl create namespace test-auto-quota || echo "Namespace already exists"
sleep 5
echo "ğŸ“Š Checking if Kyverno auto-created ResourceQuota:"
kubectl get resourcequota auto-generated-quota -n test-auto-quota || echo "â³ Quota creation in progress..."

echo ""
echo "âœ… Test completed! Key observations:"
echo "ğŸ” 1. CPU resources scaled live without container restart (NotRequired policy)"
echo "ğŸ” 2. Memory resources scaled with controlled restart (RestartContainer policy)"
echo "ğŸ” 3. Application remained available throughout (restart is fast)"
echo "ğŸ” 4. Manual quotas and limits provide essential protection"
echo "ğŸ” 5. Kyverno policies enforce governance automatically"
echo "ğŸ” 6. Policy-as-Code prevents human errors"
echo "ğŸ” 7. Auto-quota generation scales security across namespaces"
echo ""
echo "ğŸ’¡ Production insight: Memory decreases require restart for safety"
echo "âš ï¸ NEVER deploy in-place resize without proper governance!"
echo "ğŸš€ This demonstrates enterprise-ready vertical scaling!"
